# ChatBot Dinamo! üöÄ
Esta √© a implementa√ß√£o pr√°tica da Equipe Roxa no PS do PET ENG COMP. Bem-vindo ao README do nosso projeto!

---

## üìù Sobre o Projeto
O Dinamo tem como fun√ß√£o auxiliar os estudantes da UFC com o processo de experi√™ncias fora do nosso pa√≠s - interc√¢mbios e duplos diplomas.

Sabemos da dificuldade encarada pelos estudantes de reunir os dados e as informa√ß√µes necess√°rias para tal finalidade. Incri√ß√£o, documentos necess√°rios, cadeiras requisitadas, tempo m√≠nimo e m√°ximo de perman√™ncia, s√£o d√∫vidas recorrentes enfrentadas pelos interessados na √°rea.

Por isso, desenvolvemos um chatbot, do zero, que possa ajudar com informa√ß√µes precisas, baseadas nos editais atualizados, acerca de todas as experi√™ncias internacionais que a universidade pode proporcionar aos alunos. Esse √© o Dinamo!

---

## üöÄ Funcionalidades  

- üìö **Busca sem√¢ntica** em editais e documentos oficiais.  
- üéì Respostas **did√°ticas e contextualizadas** sobre processos de interc√¢mbio.  
- üîç Recupera√ß√£o de trechos relevantes dos PDFs para justificar a resposta.  
- üåê **Interface web simples** com backend em Flask.  
- üíæ Base vetorial persistida em **SQLite3 + ChromaDB**.

---

## üõ†Ô∏è Tecnologias Utilizadas  

- [Python 3.10+](https://www.python.org/)  
- [Flask](https://flask.palletsprojects.com/)
- [LangChain](https://www.langchain.com/)  
- [Chroma](https://www.trychroma.com/) (armazenamento vetorial)  
- [Ollama](https://ollama.com/) (modelos de linguagem e embeddings)  

## ‚öôÔ∏è Como rodar o projeto  

### 1. Clonar reposit√≥rio  

```bash
git clone https://github.com/davodoctoribus/dinamo
cd dinamo
```
### 2. Instalar depend√™ncias 
```bash
pip install -r requirements.txt
```

### 3. Criar base vetorial

 - Adicione as fontes em PDF na pasta `./docs/data` e rode:
```bash
ollama pull nomic-embed-text
python ./docs/scrapping.py
```
 - Os embeddings ser√£o salvos no banco vetorial na pasta `./db_intercambio`

### 4. Rode o servidor
- Como esse modelo de linguagem rodar√° em nuvem, certifique-se de ter uma conta Ollama.
```bash
 ollama pull deepseek-v3.1:671b-cloud
 python app.py
 ```
---

## üß© O que foi feito 

Este projeto utiliza o paradigma de **RAG (Retrieval-Augmented Generation)**, que combina a capacidade de racioc√≠nio e gera√ß√£o de texto dos **modelos de linguagem (LLMs)** com a confiabilidade de informa√ß√µes estruturadas extra√≠das de documentos oficiais. Na pr√°tica, os editais s√£o convertidos em texto e divididos em **chunks**, que s√£o transformados em **embeddings** (vetores num√©ricos que representam semanticamente o conte√∫do) e armazenados em uma base vetorial (ChromaDB). Quando o usu√°rio faz uma pergunta, a aplica√ß√£o calcula o embedding da pergunta e realiza uma **busca sem√¢ntica** para recuperar os chunks mais relevantes, garantindo que o modelo n√£o precise ‚Äúinventar‚Äù informa√ß√µes. Esses chunks, junto com a pergunta do usu√°rio, s√£o enviados ao modelo Ollama na nuvem, que gera uma resposta did√°tica e contextualizada. Dessa forma, a abordagem RAG permite que o chatbot forne√ßa respostas precisas e confi√°veis, mantendo a capacidade de linguagem natural e explica√ß√£o detalhada, ao mesmo tempo em que aproveita dados reais como refer√™ncia, equilibrando gera√ß√£o de texto e veracidade das informa√ß√µes.

As principais etapas foram:  

1. **Extra√ß√£o de dados**:  
   - Os editais em PDF foram processados e convertidos em texto.  
   - Esse texto foi dividido em **chunks** (fragmentos menores) para melhor indexa√ß√£o.  

2. **Cria√ß√£o de base vetorial**:  
   - Cada chunk foi transformado em um **vetor num√©rico** (embedding) usando o modelo `nomic-embed-text`.  
   - Os vetores foram armazenados em um banco **ChromaDB persistente (SQLite3)**.  

3. **Busca sem√¢ntica**:  
   - Quando o usu√°rio faz uma pergunta, a aplica√ß√£o gera o embedding da pergunta e busca os chunks mais relevantes na base vetorial via **similaridade de cosseno**.  

4. **Integra√ß√£o com LLM (Ollama Cloud)**:  
   - Os chunks recuperados s√£o enviados junto com a pergunta para o modelo de linguagem.  
   - O modelo gera uma resposta did√°tica, utilizando o contexto dos documentos reais.  

5. **API Flask**:  
   - Foi criada uma rota `/chat` que recebe perguntas via requisi√ß√µes `POST`.  
   - A API retorna uma resposta em JSON pronta para ser consumida pelo frontend.  

6. **Frontend Web**:  
   - Desenvolvido com **HTML, CSS puro e JavaScript**.  
   - O usu√°rio digita a pergunta em um campo de input e clica em "Enviar".  
   - Um **script JS** envia a pergunta para o backend Flask via **fetch API** (`POST /chat`).  
   - A resposta retornada em JSON √© exibida dinamicamente na p√°gina.  

---


